{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example_text = \"Hello there, how are you doing today? The weather is great and Python is awesome. Sentdex is crazy.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello there, how are you doing today?', 'The weather is great and Python is awesome.', 'Sentdex is crazy.']\n"
     ]
    }
   ],
   "source": [
    "print(sent_tokenize(example_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'there', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'weather', 'is', 'great', 'and', 'Python', 'is', 'awesome', '.', 'Sentdex', 'is', 'crazy', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(example_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming is a process where a given word is stripped to it's root word. This is an useful technique.. Like in the case of writting, written, or write.. the root is write. Let's say we are given two sentences:\n",
    "\n",
    "1. I am riding in the car. \n",
    "2. I am taking in a ride in the car. \n",
    "\n",
    "Both of the above sentences have the same meaning. However, have ride and riding. \n",
    "\n",
    "It's good to know about stemming but isn't really important going forward because there are better alternatives like wordnet etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer #there are other stemmers that\n",
    "# can bebe used as well\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\n",
      "pyhton\n",
      "python\n",
      "python\n",
      "pythononli\n",
      "apython\n"
     ]
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "example_words = [\"python\",\"pyhtoner\",\"pythoning\",\"pythoned\",\"pythononly\",\"apythons\"]\n",
    "for w in example_words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n",
      "convers\n",
      "AI\n",
      "team\n",
      ",\n",
      "research\n",
      "initi\n",
      "found\n",
      "jigsaw\n",
      "googl\n",
      "(\n",
      "part\n",
      "alphabet\n",
      ")\n",
      "work\n",
      "tool\n",
      "help\n",
      "improv\n",
      "onlin\n",
      "convers\n",
      ".\n",
      "one\n",
      "area\n",
      "focu\n",
      "studi\n",
      "neg\n",
      "onlin\n",
      "behavior\n",
      ",\n",
      "like\n",
      "toxic\n",
      "comment\n",
      "(\n",
      "i.e\n",
      ".\n",
      "comment\n",
      "rude\n",
      ",\n",
      "disrespect\n",
      "otherwis\n",
      "like\n",
      "make\n",
      "someon\n",
      "leav\n",
      "discuss\n",
      ")\n",
      ".\n",
      "So\n",
      "far\n",
      "’\n",
      "built\n",
      "rang\n",
      "publicli\n",
      "avail\n",
      "model\n",
      "serv\n",
      "perspect\n",
      "api\n",
      ",\n",
      "includ\n",
      "toxic\n",
      ".\n",
      "but\n",
      "current\n",
      "model\n",
      "still\n",
      "make\n",
      "error\n",
      ",\n",
      "’\n",
      "allow\n",
      "user\n",
      "select\n",
      "type\n",
      "toxic\n",
      "’\n",
      "interest\n",
      "find\n",
      "(\n",
      "e.g\n",
      ".\n",
      "platform\n",
      "may\n",
      "fine\n",
      "profan\n",
      ",\n",
      "type\n",
      "toxic\n",
      "content\n",
      ")\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "new_sentence = \"The Conversation AI team, a research initiative founded by Jigsaw and Google (both a part of Alphabet) are working on tools to help improve online conversation. One area of focus is the study of negative online behaviors, like toxic comments (i.e. comments that are rude, disrespectful or otherwise likely to make someone leave a discussion). So far they’ve built a range of publicly available models served through the Perspective API, including toxicity. But the current models still make errors, and they don’t allow users to select which types of toxicity they’re interested in finding (e.g. some platforms may be fine with profanity, but not with other types of toxic content).\"\n",
    "words = word_tokenize(new_sentence)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_sentence = [w for w in words if w not in stop_words]\n",
    "for word in filtered_sentence:\n",
    "    print(ps.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking/Chinking\n",
    "\n",
    "Generally, the named entity or the noun is the subject. There can be many nouns in a sentence. There are words surrounding these nouns that are called modifiers which tell something about the subject or the noun in question. \n",
    "\n",
    "One of the main goals of chunking is to group into what are known as \"noun phrases.\" These are phrases of one or more words that contain a noun, maybe some descriptive words, maybe a verb, and maybe something like an adverb. The idea is to group nouns with the words that are in relation to them.\n",
    "\n",
    "More here: https://pythonprogramming.net/chunking-nltk-tutorial/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Lemmatizing\n",
    "\n",
    "Lemmatizing is similar to stemming. Just that the result is some form of the complete word itself, not just the root. \n",
    "\n",
    "It's possible that you might end up with a very different word but it'll have the same meaning as the original word. \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n",
      "cactus\n",
      "goose\n",
      "rock\n",
      "reduction\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(\"cats\"))\n",
    "print(lemmatizer.lemmatize(\"cactus\"))\n",
    "print(lemmatizer.lemmatize(\"geese\"))\n",
    "print(lemmatizer.lemmatize(\"rock\"))\n",
    "print(lemmatizer.lemmatize(\"reductions\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Photoshare\n",
      "Demo\n",
      "Signup\n",
      "(\n",
      "Monday\n",
      ",\n",
      "Week\n",
      "2\n",
      "!\n",
      ")\n",
      "Hey\n",
      "everybody\n",
      ",\n",
      "I\n",
      "have\n",
      "linked\n",
      "one\n",
      "signup\n",
      "poll\n",
      "for\n",
      "the\n",
      "Photoshare\n",
      "demo\n",
      "for\n",
      "Monday\n",
      "(\n",
      "12\n",
      "03\n",
      ")\n",
      ".\n",
      "They\n",
      "are\n",
      "15\n",
      "minute\n",
      "each\n",
      "to\n",
      "hopefully\n",
      "reduction\n",
      "the\n",
      "number\n",
      "of\n",
      "demo\n",
      "that\n",
      "run\n",
      "overtime\n",
      ".\n",
      "Be\n",
      "prepared\n",
      "to\n",
      "gsubmit\n",
      "your\n",
      "assignment\n",
      "before\n",
      "you\n",
      "demo\n",
      "(\n",
      "make\n",
      "sure\n",
      "to\n",
      "name\n",
      "your\n",
      "second\n",
      "gsubmission\n",
      "something\n",
      "different\n",
      "than\n",
      "the\n",
      "first\n",
      ")\n",
      ".\n",
      "There\n",
      "will\n",
      "be\n",
      "additional\n",
      "demo\n",
      "later\n",
      "in\n",
      "the\n",
      "week\n",
      ",\n",
      "so\n",
      "do\n",
      "not\n",
      "worry\n",
      "if\n",
      "you\n",
      "have\n",
      "not\n",
      "demoed\n",
      "and\n",
      "ca\n",
      "not\n",
      "signup\n",
      "today\n",
      ".\n",
      "However\n",
      ",\n",
      "the\n",
      "vast\n",
      "majority\n",
      "of\n",
      "your\n",
      "project\n",
      "grade\n",
      "will\n",
      "come\n",
      "from\n",
      "the\n",
      "demo\n",
      "so\n",
      "make\n",
      "sure\n",
      "that\n",
      "you\n",
      "do\n",
      "sign\n",
      "up\n",
      "for\n",
      "one\n",
      "when\n",
      "available\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "#using it for sentences\n",
    "\n",
    "text = \"Photoshare Demo Signup (Monday, Week 2!) Hey everybody,I have linked one signup poll for the Photoshare demos for Monday (12 03). They are 15 minutes each to hopefully reductions the number of demos that run overtime. Be prepared to gsubmit your assignment before you demo (make sure to name your second gsubmission something different than the first). There will be additional demos later in the week, so don't worry if you haven't demoed and can't signup today. However, the vast majority of your project grade will come from the demos so make sure that you do sign up for one when available.\"\n",
    "tokenized_list = word_tokenize(text)\n",
    "\n",
    "for each in tokenized_list:\n",
    "    if each == \"n't\":\n",
    "        each = \"not\"\n",
    "    print(lemmatizer.lemmatize(each))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
