{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example_text = \"Hello there, how are you doing today? The weather is great and Python is awesome. Sentdex is crazy.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello there, how are you doing today?', 'The weather is great and Python is awesome.', 'Sentdex is crazy.']\n"
     ]
    }
   ],
   "source": [
    "print(sent_tokenize(example_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'there', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'weather', 'is', 'great', 'and', 'Python', 'is', 'awesome', '.', 'Sentdex', 'is', 'crazy', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(example_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming is a process where a given word is stripped to it's root word. This is an useful technique.. Like in the case of writting, written, or write.. the root is write. Let's say we are given two sentences:\n",
    "\n",
    "1. I am riding in the car. \n",
    "2. I am taking in a ride in the car. \n",
    "\n",
    "Both of the above sentences have the same meaning. However, have ride and riding. \n",
    "\n",
    "It's good to know about stemming but isn't really important going forward because there are better alternatives like wordnet etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer #there are other stemmers that\n",
    "# can bebe used as well\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\n",
      "pyhton\n",
      "python\n",
      "python\n",
      "pythononli\n",
      "apython\n"
     ]
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "example_words = [\"python\",\"pyhtoner\",\"pythoning\",\"pythoned\",\"pythononly\",\"apythons\"]\n",
    "for w in example_words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n",
      "convers\n",
      "AI\n",
      "team\n",
      ",\n",
      "research\n",
      "initi\n",
      "found\n",
      "jigsaw\n",
      "googl\n",
      "(\n",
      "part\n",
      "alphabet\n",
      ")\n",
      "work\n",
      "tool\n",
      "help\n",
      "improv\n",
      "onlin\n",
      "convers\n",
      ".\n",
      "one\n",
      "area\n",
      "focu\n",
      "studi\n",
      "neg\n",
      "onlin\n",
      "behavior\n",
      ",\n",
      "like\n",
      "toxic\n",
      "comment\n",
      "(\n",
      "i.e\n",
      ".\n",
      "comment\n",
      "rude\n",
      ",\n",
      "disrespect\n",
      "otherwis\n",
      "like\n",
      "make\n",
      "someon\n",
      "leav\n",
      "discuss\n",
      ")\n",
      ".\n",
      "So\n",
      "far\n",
      "’\n",
      "built\n",
      "rang\n",
      "publicli\n",
      "avail\n",
      "model\n",
      "serv\n",
      "perspect\n",
      "api\n",
      ",\n",
      "includ\n",
      "toxic\n",
      ".\n",
      "but\n",
      "current\n",
      "model\n",
      "still\n",
      "make\n",
      "error\n",
      ",\n",
      "’\n",
      "allow\n",
      "user\n",
      "select\n",
      "type\n",
      "toxic\n",
      "’\n",
      "interest\n",
      "find\n",
      "(\n",
      "e.g\n",
      ".\n",
      "platform\n",
      "may\n",
      "fine\n",
      "profan\n",
      ",\n",
      "type\n",
      "toxic\n",
      "content\n",
      ")\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "new_sentence = \"The Conversation AI team, a research initiative founded by Jigsaw and Google (both a part of Alphabet) are working on tools to help improve online conversation. One area of focus is the study of negative online behaviors, like toxic comments (i.e. comments that are rude, disrespectful or otherwise likely to make someone leave a discussion). So far they’ve built a range of publicly available models served through the Perspective API, including toxicity. But the current models still make errors, and they don’t allow users to select which types of toxicity they’re interested in finding (e.g. some platforms may be fine with profanity, but not with other types of toxic content).\"\n",
    "words = word_tokenize(new_sentence)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_sentence = [w for w in words if w not in stop_words]\n",
    "for word in filtered_sentence:\n",
    "    print(ps.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of Speech Tagging"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
